def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace('Ñ‘', 'Ðµ')
    text = text.strip()
    spec_symb = ['\\t', '\\r', '\\n', '\\w', '\\f', '\\d', '\\v']
    for symb in spec_symb:
        text = text.replace(symb, ' ')
    words = text.split()
    text = ' '.join(words)
    return text

test_case_normalize = ['ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t', 'Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°', 'Hello\r\nWorld', '  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  ']

for i in test_case_normalize:
    print(normalize(i))


def tokenize(text: str) -> list[str]:
    cleaned = text.replace(',', ' ').replace('.', ' ').replace(':', ' ').replace(';', ' ').replace('!', ' ').replace('?', ' ')
    words = cleaned.split()
    filtered = []
    for word in words:
        if not word[0].isdigit() and not word[0].isalpha():
            pass
        else:
            filtered.append(word)
    return filtered

test_case_tokenize = ['Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€', 'hello,world!!!', 'Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾', '2025 Ð³Ð¾Ð´', 'emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾']

for n in test_case_tokenize:
    print(tokenize(n))


def count_freq(tokens: list[str]) -> dict[str, int]:
    unic_t = list(set(tokens))
    count_t = []
    for i in unic_t:
        count_t.append(tokens.count(i))
    dict_t = dict(zip(unic_t,count_t))
    return dict_t

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    top = sorted(list(freq.items()), key=lambda x: (-x[1], x[0])) [:n]
    return top

test_case_count_freq =[["a","b","a","c","b","a"]]

# for n in test_case_count_freq:
#     # print(top_n(n))

